{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIm1eTVwQgLP",
        "outputId": "24aa79f9-8710-4db2-90f7-143486ac6d95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#%pip install nltk\n",
        "#%pip install --upgrade pip\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"data.pkl\", \"rb\") as f:\n",
        "    questions_matrix, data, best_num_clusters, tfidf_vectorizer,tfidf_vectorizer1 = pickle.load(f)\n",
        "    \n",
        "with open(\"model.pkl\", \"rb\") as f1:\n",
        "        model, history= pickle.load(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ1LAQVSQgLP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_question(user_question):\n",
        "    # Convertir la question en minuscules\n",
        "    user_question = user_question.lower()\n",
        "\n",
        "    # Supprimer la ponctuation\n",
        "    user_question = re.sub(r'[^\\w\\s]', '', user_question)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(user_question)\n",
        "\n",
        "    # Supprimer les mots vides (stop words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruire la question à partir des tokens lemmatisés\n",
        "    processed_question = ' '.join(tokens)\n",
        "\n",
        "    return processed_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI6qfRjEQgLQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "user_question = preprocess_question(user_question) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrainement du modèle de classification / Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_first_line(text):\n",
        "    return text.split('\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCHE89nQgLb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Fonction pour recommander des réponses avec une similarité supérieure à 40%\n",
        "def recommend_responses(new_question, model, df, tfidf_vectorizer, threshold=0.4, top_n=25):\n",
        "    new_question_vector = tfidf_vectorizer.transform([new_question])\n",
        "    new_question_vector_array = new_question_vector.toarray()\n",
        "\n",
        "    visited_clusters = set()\n",
        "    recommended_responses = []\n",
        "\n",
        "    while len(recommended_responses) < top_n and len(visited_clusters) < len(df['cluster'].unique()):\n",
        "        # Prédire le cluster de la nouvelle question\n",
        "        predicted_clusters = model.predict(new_question_vector_array)\n",
        "        \n",
        "        # Récupérer les clusters \n",
        "        ordered_clusters = np.argsort(-predicted_clusters, axis=1).flatten()\n",
        "\n",
        "        # Sélectionner le premier cluster non visité\n",
        "        for predicted_cluster in ordered_clusters:\n",
        "            if predicted_cluster not in visited_clusters:\n",
        "                visited_clusters.add(predicted_cluster)\n",
        "                break\n",
        "\n",
        "        # Récupérer les questions et réponses du cluster prédit\n",
        "        cluster_questions = df[df['cluster'] == predicted_cluster]\n",
        "\n",
        "        # Calculer la similarité cosinus entre la nouvelle question et les questions du cluster\n",
        "        cluster_question_vectors = tfidf_vectorizer.transform(cluster_questions['body_x'])\n",
        "        similarities = cosine_similarity(new_question_vector, cluster_question_vectors).flatten()\n",
        "\n",
        "        # Filtrer les réponses avec une similarité supérieure au seuil spécifié\n",
        "        high_similarity_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
        "\n",
        "        # Récupérer les réponses recommandées, leurs clusters et leurs similarités\n",
        "        cluster_recommendations = cluster_questions.iloc[high_similarity_indices][['body_y', 'cluster']].copy()\n",
        "        cluster_recommendations['similarity'] = similarities[high_similarity_indices]\n",
        "\n",
        "        # Ajouter les réponses recommandées à la liste finale\n",
        "        recommended_responses.extend(cluster_recommendations.to_dict('records'))\n",
        "\n",
        "        # Supprimer les réponses dupliquées tout en préservant l'ordre\n",
        "        seen = set()\n",
        "        unique_recommended_responses = []\n",
        "        for response in recommended_responses:\n",
        "            if response['body_y'] not in seen:\n",
        "                unique_recommended_responses.append(response)\n",
        "                seen.add(response['body_y'])\n",
        "        \n",
        "        recommended_responses = unique_recommended_responses\n",
        "\n",
        "    # Convertir la liste de réponses recommandées en DataFrame\n",
        "    recommended_df = pd.DataFrame(recommended_responses).sort_values(by='similarity', ascending=False).head(top_n)\n",
        "\n",
        "    # Extraire la liste de réponses recommandées (body_y)\n",
        "    response_list = recommended_df['body_y'].tolist()\n",
        "\n",
        "    return response_list, recommended_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenir les réponses recommandées\n",
        "recommended_responses, recommended_df = recommend_responses(user_question, model, data, tfidf_vectorizer1)\n",
        "\n",
        "similarities_data = recommended_df\n",
        "\n",
        "similarities_data['similarity'] = similarities_data['similarity'].apply(lambda x: f\"{x*100:.2f}%\")\n",
        "\n",
        "    # Convert the similarity values back to float for sorting\n",
        "similarities_data['similarity_float'] = similarities_data['similarity'].str.rstrip('%').astype(float)\n",
        "\n",
        "    # Sort the DataFrame by the similarity column in descending order\n",
        "similarities_data = similarities_data.sort_values(by='similarity_float', ascending=False)\n",
        "similarities_data = similarities_data.drop(columns=['similarity_float'])\n",
        "\n",
        "similarities_data = similarities_data.head(15)\n",
        "\n",
        "# Afficher les réponses recommandées sous forme de liste\n",
        "print(\"Recommended responses:\")\n",
        "for i, response in enumerate(recommended_responses, start=1):\n",
        "    print(f\"{i}. {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recommended_responses = pd.DataFrame(recommended_responses, columns=['Answers']).drop_duplicates()\n",
        "recommended_responses.rename(columns={'0': 'Answers'}, inplace=True)\n",
        "recommended_responses = recommended_responses.iloc[:15]\n",
        "\n",
        "print(similarities_data.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df = history_df.iloc[:15]\n",
        "\n",
        "history_df.to_csv('history.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarities_data.to_csv('data.csv',index=False)\n",
        "recommended_responses.to_csv('recommendations.csv', index= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tracer la courbe de la fonction de perte\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Courbe de la fonction de perte')\n",
        "plt.xlabel('Épochs')\n",
        "plt.ylabel('Perte')\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the accuracy\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.title('Courbe de la précision du modèle')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig('accuracy_plot.png')\n",
        "plt.legend()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
        "\n",
        "\"\"\"BufferErrory_pred = model.predict(X_train)\n",
        "y_pred_clusters = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculer la matrice de confusion\n",
        "conf_matrix = confusion_matrix(y_train, y_pred_clusters)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "# Afficher la matrice de confusion\n",
        "plt.title('Matrice de Confusion')\n",
        "plt.show()\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "dask_model",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5038318,
          "sourceId": 8453846,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30700,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
