{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <I>Preprecossing des collections:</I>\n",
    "Utilisation de NLP (Natural Language Processing) par sa bib python NLTK (Natural Language Tool Kit).\n",
    "<I><strong>Le NLP combine la linguistique computationnelle \"règle basée sur la modélisation du langage humain \"avec des modèles statistiques et d'apprentissage automatique pour permettre aux ordinateurs et aux appareils numériques, comprendre et générer du texte et de la parole.</strong></I>\n",
    "Les techniques NLP utilisées sont:\n",
    "## <I>1.Tokinisation:</I>\n",
    "processus de conversion d'une séquence de texte en parties plus petites, appelées jetons (tokens).Ces jetons peuvent être aussi petits que des caractères ou aussi longs que des mots. La principale raison pour laquelle ce processus est important est qu'il facilite l'analyse.\n",
    "[\"Chatbots\", \"are\", \"helpful\"] = [\"C\", \"h\", \"a\", \"t\", \"b\", \"o\", \"t\", \"s\", \" \", \"a\", \"r\", \"e\", \" \", \"h\", \"e\", \"l\", \"p\", \"f\", \"u\", \"l\"].\n",
    "### Types de Tokinization :\n",
    "Tokenisation de mots, Tokenisation de caractères et Tokenisation de sous-mots.\n",
    "## <I>2.Suppression des mots vides (stopwords):</I>\n",
    "Les mots-clés sont les plus courants mots dans n'importe quelle langue naturelle. Aux fins de l'analyse des données textuelles etde la création de modèles de PNL, ces mots d'arrêt peuvent ne pas ajouter beaucoup de valeur à la signification du document.\n",
    "Généralement, les mots les plus couramment utilisés dans un texte francais sont “ le ”, “ est ”, “ dans ”, “ pour ” où “, ” quand... et dans un texte anglais \"and “, \"or\", \"who\", \"which\",......La suppression des mots d'arrêt peut potentiellement aider à améliorer les performances car il reste moins de jetons significatifs. Cela pourrait augmenter la précision de la classification.\n",
    "## <I>3.Lemmatizer:</I>\n",
    "Est une technique de pré-traitement de texte utilisée dans les modèles de traitement du langage naturel (NLP) pour décomposer un mot à sa signification fondamentale afin d'identifier les similitudes. Par exemple, un algorithme de lemmatisation réduirait le mot mieux à sa racine, ou lemme(forme de dictionnaire).Par exemple, vous pouvez vous attendre à un algorithme de lemmatisation pour mapper “runs,” “running,” et “ran” au lemme, “run.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo as mongo\n",
    "\n",
    "client = mongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"admin\"]\n",
    "db = client[\"admin\"]\n",
    "\n",
    "collection_questions = db[\"questions\"]\n",
    "collection_answers = db[\"answers\"]\n",
    "\n",
    "questions_clean = db[\"questions_clean\"]\n",
    "answers_clean = db[\"answers_clean\"]\n",
    "\n",
    "#Vider les collections\n",
    "questions_clean.delete_many({})\n",
    "answers_clean.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitement terminé. Les données ont été enregistrées dans la collection questions_clean de MongoDB.\n"
     ]
    }
   ],
   "source": [
    "questions = list(collection_questions.find())\n",
    "\n",
    "def preprocess_text(question):\n",
    "    # Convertir la question en minuscules\n",
    "    question = question.lower()\n",
    "    exp = re.compile(r'<.*?>')\n",
    "    question = re.sub(exp, '', question)\n",
    "    # Supprimer la ponctuation\n",
    "    question = re.sub(r'[^\\w\\s]', '', question)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(question)\n",
    "    \n",
    "    # Supprimer les mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Reconstruire la question à partir des tokens lemmatisés\n",
    "    processed_question = ' '.join(tokens)\n",
    "    \n",
    "    return processed_question\n",
    "\n",
    "# Appliquer le prétraitement à toutes les questions\n",
    "preprocessed_data = []\n",
    "for item in questions:\n",
    "    preprocessed_item = {}\n",
    "    for key, value in item.items():\n",
    "        if key != \"link\":\n",
    "            # Appliquer le prétraitement sur les valeurs de type String\n",
    "            if isinstance(value, str):\n",
    "                preprocessed_item[key] = preprocess_text(value)\n",
    "            else:\n",
    "                preprocessed_item[key] = value\n",
    "        else:\n",
    "            preprocessed_item[key] = value\n",
    "\n",
    "        if key == \"answers\":\n",
    "            for answer in value:\n",
    "                for field_key, field_value in answer.items():\n",
    "                    if isinstance(field_value, str):\n",
    "                        answer[field_key] = preprocess_text(field_value)\n",
    "        elif isinstance(value, str):\n",
    "            item[key] = preprocess_text(value)\n",
    "\n",
    "    preprocessed_data.append(preprocessed_item)\n",
    "\n",
    "# Enregistrer les données prétraitées dans une nouvelle collection MongoDB\n",
    "db[\"questions_clean\"].insert_many(preprocessed_data)\n",
    "print(\"Prétraitement terminé. Les données ont été enregistrées dans la collection questions_clean de MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitement terminé. Les données ont été enregistrées dans la collection answers_clean de MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import string\n",
    "\n",
    "answers = list(collection_answers.find())\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def clean_answer_text(answer):\n",
    "    # Supprimer les balises HTML\n",
    "    cleaned_text = remove_html_tags(answer)\n",
    "    \n",
    "    # Supprimer les caractères de contrôle et les espaces superflus\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "    # Supprimer la ponctuation (optionnel)\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "preprocessed_data = []\n",
    "for item in answers:\n",
    "    preprocessed_item = {}\n",
    "    for key, value in item.items():\n",
    "        if key != \"link\":\n",
    "            if isinstance(value, str):\n",
    "                preprocessed_item[key] = clean_answer_text(value)\n",
    "            else:\n",
    "                preprocessed_item[key] = value\n",
    "\n",
    "        else:\n",
    "            preprocessed_item[key] = value\n",
    "\n",
    "    preprocessed_data.append(preprocessed_item)\n",
    "\n",
    "# Enregistrer les données prétraitées dans une nouvelle collection MongoDB\n",
    "db[\"answers_clean\"].insert_many(preprocessed_data)\n",
    "print(\"Prétraitement terminé. Les données ont été enregistrées dans la collection answers_clean de MongoDB.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
