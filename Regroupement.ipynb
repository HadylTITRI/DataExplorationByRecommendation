{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "ca = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo as mongo\n",
    "import pandas as pd\n",
    "\n",
    "#client = mongo.MongoClient(\"mongodb+srv://hadyltitri:QScSgXpsINfxAfQC@cluster0.zvbmwjb.mongodb.net/xyzdb?retryWrites=true&w=majority\", tlsCAFile=ca)\n",
    "client = mongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"admin\"]\n",
    "\n",
    "collection_qst = db[\"questions_clean\"]\n",
    "collection_ans = db[\"answers_clean\"]\n",
    "\n",
    "# Identifier et supprimer les doublons basés sur l'attribut 'answer_id' dans la collection answers_clean\n",
    "duplicates = collection_ans.aggregate([\n",
    "    { '$group': { '_id': '$answer_id', 'duplicates': { '$addToSet': '$_id' }, 'count': { '$sum': 1 } } },\n",
    "    { '$match': { 'count': { '$gt': 1 } } }\n",
    "])\n",
    "\n",
    "for doc in duplicates:\n",
    "    doc['duplicates'].pop(0)  # Conserver un élément, supprimer les autres\n",
    "    collection_ans.delete_many({ '_id': { '$in': doc['duplicates'] } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              body_x  \\\n",
      "0  want bake chicken chocolate chip muffin overla...   \n",
      "1  want bake chicken chocolate chip muffin overla...   \n",
      "\n",
      "                                              body_y  \n",
      "0  I would say no If you think in common sense me...  \n",
      "1  It is absolutely safe to do so For most foods ...  \n"
     ]
    }
   ],
   "source": [
    "questions_df = list(collection_qst.find({}))\n",
    "questions_df = pd.json_normalize(questions_df)\n",
    "df_questions = pd.DataFrame(questions_df)\n",
    "\n",
    "answers_df = list(collection_ans.find({}))\n",
    "answers_df = pd.json_normalize(answers_df)\n",
    "df_answers = pd.DataFrame(answers_df)\n",
    "\n",
    "df_merged = pd.merge(df_questions,df_answers,on='question_id',how='inner')\n",
    "\n",
    "data = df_merged[['body_x', 'body_y']]\n",
    "print(data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Vectorisation des textes\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "questions_matrix = tfidf_vectorizer.fit_transform(data['body_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraire les noms des fonctionnalités/Le vocabumaire extraits des questions\n",
    "tfidf_tokens = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:993: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 980, in _score\n",
      "    scores = scorer(estimator, X_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Paramètres pour GridSearchCV\n",
    "param_grid = {'n_clusters': range(7, 15)}\n",
    "\n",
    "# Tester différents nombres de clusters\n",
    "# Créer un scorer pour la métrique silhouette\n",
    "silhouette_scorer = make_scorer(silhouette_score)\n",
    "\n",
    "# Recherche par grille pour le nombre de clusters avec KMeans\n",
    "kmeans = KMeans(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(kmeans, param_grid, scoring=silhouette_scorer)\n",
    "grid_search.fit(questions_matrix)\n",
    "best_num_clusters = grid_search.best_params_['n_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans(n_clusters=best_num_clusters, random_state=42)\n",
    "question_clusters = kmeans_clustering.fit_predict(questions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_14608\\2229855281.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cluster'] = question_clusters\n"
     ]
    }
   ],
   "source": [
    "cluster_label = kmeans_clustering.labels_\n",
    "data['cluster'] = question_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraiter les questions\n",
    "tfidf_vectorizer1 = TfidfVectorizer(max_features=15000)  \n",
    "tfidf_matrix = tfidf_vectorizer1.fit_transform(data['body_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Créer et entraîner un modèle Keras\n",
    "input_dim = tfidf_matrix.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.001)))#256: le nombre de neurones, input_dim : nombre de features (vecteurs) que cette couche attend en entrée\n",
    "model.add(Dropout(0.5)) # 50%=0.5 : pourcentage de neurones à supprimer ou abondonner pour réduire l'overfitting.\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(best_num_clusters, activation='softmax'))  \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 35ms/step - accuracy: 0.8343 - loss: 0.7547 - val_accuracy: 0.9783 - val_loss: 0.3078\n",
      "Epoch 2/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 36ms/step - accuracy: 0.9623 - loss: 0.3500 - val_accuracy: 0.9798 - val_loss: 0.2757\n",
      "Epoch 3/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 29ms/step - accuracy: 0.9665 - loss: 0.3182 - val_accuracy: 0.9827 - val_loss: 0.2610\n",
      "Epoch 4/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 29ms/step - accuracy: 0.9697 - loss: 0.2989 - val_accuracy: 0.9840 - val_loss: 0.2502\n",
      "Epoch 5/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 29ms/step - accuracy: 0.9716 - loss: 0.2852 - val_accuracy: 0.9845 - val_loss: 0.2453\n",
      "Epoch 6/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 31ms/step - accuracy: 0.9732 - loss: 0.2796 - val_accuracy: 0.9847 - val_loss: 0.2420\n",
      "Epoch 7/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 27ms/step - accuracy: 0.9745 - loss: 0.2735 - val_accuracy: 0.9863 - val_loss: 0.2397\n",
      "Epoch 8/8\n",
      "\u001b[1m2715/2715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 29ms/step - accuracy: 0.9742 - loss: 0.2726 - val_accuracy: 0.9894 - val_loss: 0.2304\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "X_train = tfidf_matrix.toarray()\n",
    "y_train = data['cluster'].values\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=8, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump([questions_matrix, data, best_num_clusters, tfidf_vectorizer,tfidf_vectorizer1], f)\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f1:\n",
    "    pickle.dump([ model, history], f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
