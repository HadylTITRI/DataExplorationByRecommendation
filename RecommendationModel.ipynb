{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIm1eTVwQgLP",
        "outputId": "24aa79f9-8710-4db2-90f7-143486ac6d95",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#%pip install nltk\n",
        "#%pip install --upgrade pip\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"data.pkl\", \"rb\") as f:\n",
        "    questions_matrix, data, best_num_clusters, tfidf_vectorizer, model, history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ1LAQVSQgLP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_question(user_question):\n",
        "    # Convertir la question en minuscules\n",
        "    user_question = user_question.lower()\n",
        "\n",
        "    # Supprimer la ponctuation\n",
        "    user_question = re.sub(r'[^\\w\\s]', '', user_question)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(user_question)\n",
        "\n",
        "    # Supprimer les mots vides (stop words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruire la question à partir des tokens lemmatisés\n",
        "    processed_question = ' '.join(tokens)\n",
        "\n",
        "    return processed_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI6qfRjEQgLQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "user_question = preprocess_question(user_question) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrainement sans Modèle pour tester (Cette méthode est valide dans note cas d'étude,car le nombre de données n'est pas massive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_question_vector = tfidf_vectorizer.transform([user_question])\n",
        "\n",
        "print(user_question_vector.shape)\n",
        "print(questions_matrix.shape)\n",
        "\n",
        "cluster_similarities = cosine_similarity(user_question_vector, questions_matrix).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cosine_similarities = cosine_similarity(user_question_vector, questions_matrix).flatten()\n",
        "similarities_data = pd.DataFrame({\n",
        "    'question_id': data['question_id'],\n",
        "    'similarity': cosine_similarities,\n",
        "    'cluster': data['cluster'],\n",
        "    'answer_id': data['answer_id']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarities_data.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Top 10 similar questions:\")\n",
        "print(similarities_data.sort_values(by='similarity', ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trouver les questions les plus similaires\n",
        "most_similar_question = similarities_data.loc[similarities_data['similarity'].idxmax()]\n",
        "\n",
        "# Cluster des questions les plus similaires\n",
        "predicted_cluster = most_similar_question['cluster']\n",
        "print(f\"Predicted cluster for the new question: {predicted_cluster}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Récupération des réponses du cluster prédit\n",
        "cluster_responses = data[data['cluster'] == predicted_cluster]\n",
        "\n",
        "# Calcul de la similarité entre la nouvelle question et les questions du cluster\n",
        "cluster_cosine_similarities = cosine_similarity(user_question_vector, tfidf_vectorizer.transform(cluster_responses['body_x'])).flatten()\n",
        "\n",
        "# Ajout des similarités au DataFrame du cluster\n",
        "cluster_responses = cluster_responses.assign(similarity=cluster_cosine_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Récupérer les indices des réponses les plus similaires dans le cluster\n",
        "most_similar_indices_in_cluster = cluster_responses['similarity'].nlargest(15).index\n",
        "\n",
        "# Récupérer les réponses recommandées dans une liste\n",
        "recommended_responses = cluster_responses.loc[most_similar_indices_in_cluster, 'body_y'].tolist()\n",
        "\n",
        "# Supprimer les réponses dupliquées tout en préservant l'ordre\n",
        "seen = set()\n",
        "unique_recommended_responses = []\n",
        "for response in recommended_responses:\n",
        "    if response not in seen:\n",
        "        unique_recommended_responses.append(response)\n",
        "        seen.add(response)\n",
        "\n",
        "# Afficher les réponses recommandées sous forme de liste\n",
        "print(\"Les réponses recommandées sont:\\n:\")\n",
        "for i, response in enumerate(unique_recommended_responses, start=1):\n",
        "    print(f\"{i}. {response}\"+\"\\n*************************************************\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir les similarités en DataFrame pour une meilleure visualisation\n",
        "similarities_df = pd.DataFrame(cluster_similarities.T, index=data.index, columns=['similarity'])\n",
        "\n",
        "# Ajouter les similarités au DataFrame original\n",
        "data = data.join(similarities_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identifier le cluster de la question la plus similaire\n",
        "most_similar_question_idx = data['similarity'].idxmax()\n",
        "predicted_cluster = data.loc[most_similar_question_idx, 'cluster']\n",
        "print(predicted_cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sélectionner les réponses des questions du même cluster avec une similarité > 80%\n",
        "recommended_responses = data[(data['cluster'] == predicted_cluster) & (data['similarity'] >= 0.5)][['answer_id', 'body_y']]\n",
        "recommended_answers = pd.DataFrame(recommended_responses).drop_duplicates()\n",
        "\n",
        "print(\"Les réponses recommandées sont:\\n\",recommended_answers['body_y'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = data[['answer_id','view_count', 'cluster', 'is_answered', 'similarity']]\n",
        "df['similarity'] = df['similarity'].apply(lambda x: f\"{x*100:.2f}%\")\n",
        "\n",
        "# Convert the similarity values back to float for sorting\n",
        "df['similarity_float'] = df['similarity'].str.rstrip('%').astype(float)\n",
        "\n",
        "# Sort the DataFrame by the similarity column in descending order\n",
        "df = df.sort_values(by='similarity_float', ascending=False)\n",
        "df = df.drop(columns=['similarity_float'])\n",
        "\n",
        "df = df.head(20)\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrainement du modèle de classification / Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s0leLBwQgLa",
        "outputId": "4b073918-899d-479d-ff8d-a9d545fa6ce1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Prétraiter les questions\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20000)  \n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['body_x'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer et entraîner un modèle Keras\n",
        "input_dim = tfidf_matrix.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.001)))#256: le nombre de neurones, input_dim : nombre de features (vecteurs) que cette couche attend en entrée\n",
        "#model.add(Dropout(0.3)) # 50%=0.5 : pourcentage de neurones à supprimer ou abondonner pour réduire l'overfitting.\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(best_num_clusters, activation='softmax'))  \n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entraîner le modèle\n",
        "X_train = tfidf_matrix.toarray()\n",
        "y_train = data['cluster'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=8, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCHE89nQgLb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Fonction pour recommander des réponses avec une similarité supérieure à 40%\n",
        "def recommend_responses(new_question, model, df, tfidf_vectorizer, threshold=0.4, top_n=15):\n",
        "    # Prétraiter la nouvelle question\n",
        "    new_question_vector = tfidf_vectorizer.transform([new_question])\n",
        "\n",
        "    # Prédire le cluster de la nouvelle question\n",
        "    new_question_vector_array = new_question_vector.toarray()\n",
        "    predicted_cluster = np.argmax(model.predict(new_question_vector_array), axis=1)[0]\n",
        "\n",
        "    # Récupérer les questions et réponses du cluster prédit\n",
        "    cluster_questions = df[df['cluster'] == predicted_cluster]\n",
        "\n",
        "    # Calculer la similarité cosinus entre la nouvelle question et les questions du cluster\n",
        "    cluster_question_vectors = tfidf_vectorizer.transform(cluster_questions['body_x'])\n",
        "    similarities = cosine_similarity(new_question_vector, cluster_question_vectors).flatten()\n",
        "    ########################\n",
        "    #flatten():Retourne une copie d'un tableau donné de manière à ce qu'il soit réduit à une seule dimension.\n",
        "    #Cela signifie que tous les éléments du tableau seront regroupés dans un seul tableau unidimensionnel\n",
        "    #Exp: 1 2\n",
        "    #     3 4\n",
        "    # ==> 1 2 3 4\n",
        "    ########################\n",
        "    \n",
        "    # Filtrer les réponses avec une similarité supérieure au seuil spécifié\n",
        "    high_similarity_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
        "\n",
        "    # Récupérer les réponses recommandées dans une liste\n",
        "    recommended_responses = cluster_questions.iloc[high_similarity_indices]['body_y'].tolist()\n",
        "\n",
        "    # Supprimer les réponses dupliquées tout en préservant l'ordre\n",
        "    seen = set()\n",
        "    unique_recommended_responses = []\n",
        "    for response in recommended_responses:\n",
        "        if response not in seen:\n",
        "            unique_recommended_responses.append(response)\n",
        "            seen.add(response)\n",
        "\n",
        "    return unique_recommended_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df = history_df.iloc[:15]\n",
        "\n",
        "history_df.to_csv('history.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenir les réponses recommandées\n",
        "recommended_responses = recommend_responses(user_question, model, data, tfidf_vectorizer)\n",
        "\n",
        "# Afficher les réponses recommandées sous forme de liste\n",
        "print(\"Recommended responses:\")\n",
        "for i, response in enumerate(recommended_responses, start=1):\n",
        "    print(f\"{i}. {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recommended_responses = pd.DataFrame(recommended_responses, columns=['Answers'])\n",
        "recommended_responses.rename(columns={'0': 'Answers'}, inplace=True)\n",
        "recommended_responses = recommended_responses.iloc[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recommended_responses.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('data.csv',index=False)\n",
        "recommended_responses.to_csv('recommendations.csv', index= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tracer la courbe de la fonction de perte\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='Entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Courbe de la fonction de perte')\n",
        "plt.xlabel('Épochs')\n",
        "plt.ylabel('Perte')\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the accuracy\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.title('Courbe de la précision du modèle')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig('accuracy_plot.png')\n",
        "plt.legend()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = model.predict(X_train)\n",
        "y_pred_clusters = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculer la matrice de confusion\n",
        "conf_matrix = confusion_matrix(y_train, y_pred_clusters)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "# Afficher la matrice de confusion\n",
        "plt.title('Matrice de Confusion')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "dask_model",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5038318,
          "sourceId": 8453846,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30700,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
